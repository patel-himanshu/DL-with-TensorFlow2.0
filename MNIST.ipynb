{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Shape: (28, 28, 1) <br>\n",
    "Number of nodes in input layer: 784 = 28 * 28 <br>\n",
    "Number of nodes in output layer: 10 <br>\n",
    "https://www.tensorflow.org/datasets/catalog/mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.layers import Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data with 3 different options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <DatasetV1Adapter shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>,\n",
       " 'train': <DatasetV1Adapter shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data0 = tfds.load(name = 'mnist')\n",
    "data0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>,\n",
       " 'train': <DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = tfds.load(name = 'mnist', as_supervised = True)\n",
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`as_supervised = False` loads the data set in form of dictionary, with keys `image` and `label`. <br>\n",
    "`as_supervised = True` loas the data set in form of 2-tuple structure, i.e. **(image, label)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'test': <DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>,\n",
       "  'train': <DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>},\n",
       " tfds.core.DatasetInfo(\n",
       "     name='mnist',\n",
       "     version=3.0.0,\n",
       "     description='The MNIST database of handwritten digits.',\n",
       "     homepage='http://yann.lecun.com/exdb/mnist/',\n",
       "     features=FeaturesDict({\n",
       "         'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
       "         'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
       "     }),\n",
       "     total_num_examples=70000,\n",
       "     splits={\n",
       "         'test': 10000,\n",
       "         'train': 60000,\n",
       "     },\n",
       "     supervised_keys=('image', 'label'),\n",
       "     citation=\"\"\"@article{lecun2010mnist,\n",
       "       title={MNIST handwritten digit database},\n",
       "       author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "       journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n",
       "       volume={2},\n",
       "       year={2010}\n",
       "     }\"\"\",\n",
       "     redistribution_info=,\n",
       " ))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = tfds.load(name = 'mnist', as_supervised = True, with_info = True)\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>\n",
      "<DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>\n",
      "<DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "print(data0['test'])\n",
    "print(data1['test'])\n",
    "print(data2[0]['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, info = data2[0], data2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': <DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>, 'train': <DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>}\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='mnist',\n",
      "    version=3.0.0,\n",
      "    description='The MNIST database of handwritten digits.',\n",
      "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
      "    }),\n",
      "    total_num_examples=70000,\n",
      "    splits={\n",
      "        'test': 10000,\n",
      "        'train': 60000,\n",
      "    },\n",
      "    supervised_keys=('image', 'label'),\n",
      "    citation=\"\"\"@article{lecun2010mnist,\n",
      "      title={MNIST handwritten digit database},\n",
      "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
      "      journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n",
      "      volume={2},\n",
      "      year={2010}\n",
      "    }\"\"\",\n",
      "    redistribution_info=,\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data['train'], data['test'] # Storing \"train\" and \"test\" data in 2 variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfds.core.SplitInfo num_examples=60000>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.splits['train'] # Accessing \"splits\" option of \"info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.splits['train'].num_examples # Accessing the number of samples present in \"train\" data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing the number of samples in training, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 0.9 * info.splits['train'].num_examples # Storing 90% of \"train\" data for training\n",
    "num_train = tf.cast(num_train, tf.int64) # Typecasting the above value to an int (Rounding off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_validation = 0.1 * info.splits['train'].num_examples # Storing 10% of \"train\" data for validation\n",
    "num_validation = tf.cast(num_validation, tf.int64) # Typecasting the above value to an int (Rounding off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = info.splits['test'].num_examples # Storing the number of \"test\" data (Already an integer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the image to get result between 0 to 1\n",
    "# All MNIST images have values between 0 to 255. Hence dividing each value by 255.\n",
    "\n",
    "def scaling_func(image, label):\n",
    "    image = tf.cast(image, tf.float32) # Typecasting all available images into \"float32\" type \n",
    "    image = image / 255. # To ensure that we get a float as a result\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_validation = train.map(scaling_func) # Applying scaling function on training and validation data\n",
    "scaled_test = test.map(scaling_func) # Applyting scaling function on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffling all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 6000 # Number of samples which will be shuffled at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling training and validation data, based on given buffer size\n",
    "shuffled_train_validation = scaled_train_validation.shuffle(buffer_size) \n",
    "shuffled_test = scaled_test.shuffle(buffer_size) # Shuffling test data, based on given buffer size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating training, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = shuffled_train_validation.take(num_validation) # Finally, creating validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = shuffled_train_validation.skip(num_validation) # Finally, creating training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.take(x)` will take the first \"x\" images for validation. <br>\n",
    "`.skip(x)` will skip the first \"x\" images, and take all the remaining data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = shuffled_test.take(num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100 # Number of data samples, present in a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = training.batch(batch_size) # Training data needs to be batched for optimal usage of computing power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = validation.batch(num_validation) \n",
    "# Validation data is not required to be batched\n",
    "# But, the model expects the validation data set in batch form\n",
    "# Hence, creating only a single batch for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = testing.batch(num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping validation data to match that of training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_inputs, validation_targets = next(iter(validation))\n",
    "# `iter` makes the \"validation\" object iterable\n",
    "# `next` iterates \"validation\" object, and unpacks its 2 tuples into \"validation_inputs\" and \"validation_targets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size = 784 # Number of nodes in input layer\n",
    "output_layer_size = 10 # Number of nodes in each hidden layer of the model\n",
    "hidden_layer_size = 100 # Number of nodes in output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape = (28, 28, 1)))\n",
    "model.add(Dense(hidden_layer_size, activation = 'relu'))\n",
    "model.add(Dense(hidden_layer_size, activation = 'relu'))\n",
    "model.add(Dense(output_layer_size, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 89,610\n",
      "Trainable params: 89,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary() # Gives a brief summary of our model, with additional data about number of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting loss function and optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting different hyperparameters for SGD optimizer to be used in the model\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate = 0.1, momentum = 0.9, nesterov = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5 # Number of epochs we want to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 [==============================] - 4s 8ms/step - loss: 0.0382 - accuracy: 0.9899 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "540/540 [==============================] - 3s 6ms/step - loss: 0.0344 - accuracy: 0.9909 - val_loss: 0.0626 - val_accuracy: 0.9827\n",
      "Epoch 3/5\n",
      "540/540 [==============================] - 4s 6ms/step - loss: 0.0330 - accuracy: 0.9915 - val_loss: 0.0615 - val_accuracy: 0.9832\n",
      "Epoch 4/5\n",
      "540/540 [==============================] - 4s 7ms/step - loss: 0.0329 - accuracy: 0.9915 - val_loss: 0.0607 - val_accuracy: 0.9830\n",
      "Epoch 5/5\n",
      "540/540 [==============================] - 4s 7ms/step - loss: 0.0323 - accuracy: 0.9915 - val_loss: 0.0597 - val_accuracy: 0.9842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9670167990>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(training,\n",
    "          epochs = num_epochs,\n",
    "          validation_data = (validation_inputs, validation_targets),\n",
    "          verbose = 1,\n",
    "          validation_steps = 10\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 465ms/step - loss: 0.0690 - accuracy: 0.9780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06898871064186096, 0.978]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 472ms/step - loss: 0.0690 - accuracy: 0.9780\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(testing) # Tuple unpacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  0.06898871064186096\n",
      "Test Accuracy:  97.79999852180481 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Loss: \", test_loss)\n",
    "print(\"Test Accuracy: \", 100* test_accuracy, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
